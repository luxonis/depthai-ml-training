{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.luxonis.com/logo.svg\" width=\"400\">\n",
    "\n",
    "# Training a Luxonis-Train Model Using a **Custom Data Loader**\n",
    "\n",
    "## üåü Overview\n",
    "In this tutorial, the spotlight is on using a **custom data loader**.  \n",
    "You'll learn how to create and integrate a custom loader to train a model with `luxonis-train`‚Äîideal when your dataset doesn't follow standard formats or requires special preprocessing.\n",
    "\n",
    "We‚Äôll cover:\n",
    "- Creating your own **custom Loader**\n",
    "- Configuring the training pipeline\n",
    "- Running training with `luxonis-train` using your Loader\n",
    "\n",
    "## üìú Table of Contents\n",
    "- [üõ†Ô∏è Installation](#Ô∏èinstallation)\n",
    "- [üóÉÔ∏è Data Preparation](#data-preparation)  \n",
    "  - [üì• Download COCO People Subset Dataset](#download-coco-people-subset-dataset)  \n",
    "  - [üé® Creating a **Custom Loader**](#creating-a-custom-loader)\n",
    "  - [üßê Inspecting Dataset using a **Custom Loader**](#inspecting-dataset-using-a-custom-loader)\n",
    "- [üèãÔ∏è‚Äç‚ôÇÔ∏è Training](#training)  \n",
    "  - [‚öôÔ∏è Configuration](#configuration)  \n",
    "  - [ü¶æ Train](#train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Ô∏èinstallation\"></a>\n",
    "\n",
    "## üõ†Ô∏è Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The main focus of this tutorial is using [`LuxonisTrain`](https://github.com/luxonis/luxonis-train), a user-friendly tool designed to streamline the training of deep learning models, especially for edge devices. We'll also use [`LuxonisML`](https://github.com/luxonis/luxonis-ml) since it provides us with a collection of utility functionality and an easy way of creating and managing computer vision datasets called `LuxonisDataset`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q luxonis-train>=0.3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"data-preparation\"></a>\n",
    "\n",
    "## üóÉÔ∏è Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"download-coco-people-subset-dataset\"></a>\n",
    "\n",
    "### üì• Download VOCDetection dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will download the VOC detection dataset, which we will be working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\"\n",
    "output_dir = \"./data\"\n",
    "output_path = os.path.join(output_dir, \"VOCtrainval_06-Nov-2007.tar\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "# Extract the tar file\n",
    "with tarfile.open(output_path) as tar:\n",
    "    tar.extractall(path=output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"creating-a-custom-loader\"></a>\n",
    "\n",
    "### üé® Creating a Custom Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ **Implementing a Model Loader using `BaseLoaderTorch`**\n",
    "\n",
    "When creating a custom data loader class by extending [`BaseLoaderTorch`](https://github.com/luxonis/luxonis-train/blob/d8a4e5b090a4806ce12f4d21ffa1cd5a41ad48dc/luxonis_train/loaders/base_loader.py), there are specific methods and properties that you **must implement** to ensure the class functions correctly within the Luxonis Train framework. Here's a concise overview of the requirements, using `VOCLoaderTorch` as an example:\n",
    "\n",
    "### Required Methods to Implement:\n",
    "\n",
    "- `__init__`:\n",
    "  - Must call `super().__init__(**kwargs)` to initialize the base class properly.\n",
    "\n",
    "- `__len__(self) -> int`:\n",
    "  - Must return the total number of samples available in the dataset.\n",
    "\n",
    "- `get(self, idx: int) -> tuple[Tensor | dict[str, Tensor], Labels]`:\n",
    "  - Responsible for fetching and preprocessing a single sample (image and labels) from the dataset.\n",
    "  - Includes reading images, parsing annotations, applying resizing, augmentations, and returning formatted tensors.\n",
    "  - **Important**: The returned labels must follow the required `Labels` format expected by Luxonis-Train. For example:\n",
    "\n",
    "    ```python\n",
    "    labels: Labels = {\n",
    "        \"/boundingbox\": torch.tensor(bboxes, dtype=torch.float32),  # [N,5] where 5 is [cls_id, xmin, ymin, xmax, ymax], with coordinates normalized to [0,1]\n",
    "        \"/classification\": one_hot,  # [num_classes]\n",
    "    }\n",
    "\n",
    "    return img_tensor, labels\n",
    "    ```\n",
    "\n",
    "  For more supported annotation formats, refer to the documentation here: [Luxonis-Train Loaders](https://github.com/luxonis/luxonis-train/tree/main/luxonis_train/loaders#loaders)\n",
    "\n",
    "\n",
    "- `get_classes(self) -> dict[str, dict[str, int]]`:\n",
    "  - Must return a dictionary structured by **task names**.\n",
    "  - The default task name key must be `\"\"`.\n",
    "  - Each entry should map class names to integer IDs. For example:\n",
    "\n",
    "    ```python\n",
    "    def get_classes(self) -> dict[str, dict[str, int]]:\n",
    "        voc_classes = [\n",
    "            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
    "            'bus', 'car', 'cat', 'chair', 'cow',\n",
    "            'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "            'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "        ]\n",
    "        return {'': {name: idx for idx, name in enumerate(voc_classes)}}\n",
    "    ```\n",
    "\n",
    "    This format ensures the dataset is structured according to task-based expectations in Luxonis-Train.\n",
    "\n",
    "\n",
    "- `input_shapes(self) -> dict[str, torch.Size]`:\n",
    "  - Defines the shape of input tensors without batch dimensions.\n",
    "  - **Important**: The dictionary key must be `\"image\"`, representing the input image. For example:\n",
    "\n",
    "    ```python\n",
    "    @property\n",
    "    def input_shapes(self) -> dict[str, torch.Size]:\n",
    "        return {\"image\": torch.Size([3, self.height, self.width])}\n",
    "    ```\n",
    "\n",
    "\n",
    "### Example Implementation Highlights (`VOCLoaderTorch`):\n",
    "\n",
    "- **Data preparation**: Parsing XML annotations and mapping bounding boxes correctly to resized image dimensions.\n",
    "- **Augmentations**: Implementing random horizontal flips, color jittering, grayscale conversions.\n",
    "- **Normalization**: Converting images to tensors and normalizing them to standard mean and standard deviation.\n",
    "\n",
    "**Following these guidelines ensures that your loader integrates seamlessly into training pipelines, enabling model training, visualization and metric logging during training, and final model export for deployment on cameras.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from luxonis_ml.typing import Labels\n",
    "from luxonis_train.registry import LOADERS\n",
    "\n",
    "from luxonis_train import BaseLoaderTorch\n",
    "\n",
    "class VOCLoaderTorch(BaseLoaderTorch):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str = \"./data/VOCdevkit\",\n",
    "        year: str = \"2007\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.root = root\n",
    "        self.year = year\n",
    "        self.is_training = \"train\" == self.view[0]\n",
    "\n",
    "        # Get VOC classes and create mapping\n",
    "        self.class_map = self.get_classes()[\"\"]\n",
    "        self.class_name_to_id = {name: idx for idx, name in enumerate(self.class_map)}\n",
    "\n",
    "        # Collect image and annotation paths\n",
    "        self.image_ids = []\n",
    "        self.image_paths = []\n",
    "        self.annotation_paths = []\n",
    "        \n",
    "        # Load image set\n",
    "        image_set_file = os.path.join(\n",
    "            self.root, f\"VOC{year}\", \"ImageSets\", \"Main\", f\"{self.view[0]}.txt\"\n",
    "        )\n",
    "        with open(image_set_file) as f:\n",
    "            for line in f:\n",
    "                image_id = line.strip().split()[0]\n",
    "                self.image_ids.append(image_id)\n",
    "\n",
    "        # Prepare paths\n",
    "        for img_id in self.image_ids:\n",
    "            img_path = os.path.join(\n",
    "                self.root, f\"VOC{year}\", \"JPEGImages\", f\"{img_id}.jpg\"\n",
    "            )\n",
    "            ann_path = os.path.join(\n",
    "                self.root, f\"VOC{year}\", \"Annotations\", f\"{img_id}.xml\"\n",
    "            )\n",
    "            self.image_paths.append(img_path)\n",
    "            self.annotation_paths.append(ann_path)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def get(self, idx: int) -> tuple[torch.Tensor, Labels]:\n",
    "        # Read image\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = self.read_image(img_path)\n",
    "\n",
    "        # Parse annotation\n",
    "        tree = ET.parse(self.annotation_paths[idx])\n",
    "        root_xml = tree.getroot()\n",
    "\n",
    "        # Get original dimensions\n",
    "        size = root_xml.find(\"size\")\n",
    "        original_height = int(size.find(\"height\").text)\n",
    "        original_width = int(size.find(\"width\").text)\n",
    "\n",
    "        # Extract objects\n",
    "        objects = []\n",
    "        for obj in root_xml.findall(\"object\"):\n",
    "            name = obj.find(\"name\").text\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = float(bbox.find(\"xmin\").text)\n",
    "            ymin = float(bbox.find(\"ymin\").text)\n",
    "            xmax = float(bbox.find(\"xmax\").text)\n",
    "            ymax = float(bbox.find(\"ymax\").text)\n",
    "            objects.append({\n",
    "                \"name\": name,\n",
    "                \"xmin\": xmin,\n",
    "                \"ymin\": ymin,\n",
    "                \"xmax\": xmax,\n",
    "                \"ymax\": ymax,\n",
    "            })\n",
    "\n",
    "        # Letterbox resize\n",
    "        # Compute scale and new size\n",
    "        scale = min(self.width / original_width, self.height / original_height)\n",
    "        new_w = int(original_width * scale)\n",
    "        new_h = int(original_height * scale)\n",
    "        # Resize image to new size\n",
    "        resized_img = cv2.resize(img, (new_w, new_h))\n",
    "        # Compute padding\n",
    "        pad_w = self.width - new_w\n",
    "        pad_h = self.height - new_h\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        # Apply padding (using a constant value, e.g., 128)\n",
    "        img = cv2.copyMakeBorder(resized_img, pad_top, pad_bottom, pad_left, pad_right,\n",
    "                                 cv2.BORDER_CONSTANT, value=(128, 128, 128))\n",
    "\n",
    "        # Apply augmentations\n",
    "        flip = False\n",
    "        if self.is_training:\n",
    "            # Color jitter\n",
    "            if random.random() < 0.5:\n",
    "                img = self.apply_color_jitter(img)\n",
    "\n",
    "            # Random grayscale\n",
    "            if random.random() < 0.1:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            # Horizontal flip\n",
    "            if random.random() < 0.5:\n",
    "                img = cv2.flip(img, 1)\n",
    "                flip = True\n",
    "\n",
    "        # Process bounding boxes using letterbox adjustments\n",
    "        bboxes = []\n",
    "        class_ids = []\n",
    "        for obj in objects:\n",
    "            xmin = obj[\"xmin\"]\n",
    "            ymin = obj[\"ymin\"]\n",
    "            xmax = obj[\"xmax\"]\n",
    "            ymax = obj[\"ymax\"]\n",
    "\n",
    "            # Scale coordinates with letterbox scale and add padding offset\n",
    "            xmin = xmin * scale + pad_left\n",
    "            xmax = xmax * scale + pad_left\n",
    "            ymin = ymin * scale + pad_top\n",
    "            ymax = ymax * scale + pad_top\n",
    "\n",
    "            # Apply flip if required\n",
    "            if flip:\n",
    "                xmin, xmax = self.width - xmax, self.width - xmin\n",
    "\n",
    "            # Normalize to [0,1]\n",
    "            xmin /= self.width\n",
    "            xmax /= self.width\n",
    "            ymin /= self.height\n",
    "            ymax /= self.height\n",
    "\n",
    "            cls_id = self.class_name_to_id[obj[\"name\"]]\n",
    "            bboxes.append([cls_id, xmin, ymin, xmax, ymax])\n",
    "            class_ids.append(cls_id)\n",
    "\n",
    "        # Convert to tensor and normalize image\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        img_tensor = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )(img_tensor)\n",
    "\n",
    "        # Compute one-hot encoded vector for classification\n",
    "        num_classes = len(self.class_map)\n",
    "        one_hot = torch.zeros(num_classes, dtype=torch.int64)\n",
    "        for cls in class_ids:\n",
    "            one_hot[cls] = 1\n",
    "\n",
    "        # Prepare labels\n",
    "        labels: Labels = {\n",
    "            \"/boundingbox\": torch.tensor(bboxes, dtype=torch.float32),  # [cls_id, xmin, ymin, xmax, ymax]\n",
    "            \"/classification\": one_hot,  # [num_classes]\n",
    "        }\n",
    "\n",
    "        return img_tensor, labels\n",
    "\n",
    "    def get_classes(self) -> dict[str, dict[str, int]]:\n",
    "        voc_classes = [\n",
    "            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
    "            'bus', 'car', 'cat', 'chair', 'cow',\n",
    "            'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "            'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "        ]\n",
    "        return {'': {name: idx for idx, name in enumerate(voc_classes)}} \n",
    "\n",
    "    @property\n",
    "    def input_shapes(self) -> dict[str, torch.Size]:\n",
    "        return {\"image\": torch.Size([3, self.height, self.width])}\n",
    "\n",
    "    def apply_color_jitter(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Applies random color jitter to image\"\"\"\n",
    "        # Random brightness, contrast, and saturation\n",
    "        brightness = random.uniform(0.8, 1.2)\n",
    "        contrast = random.uniform(0.8, 1.2)\n",
    "        saturation = random.uniform(0.8, 1.2)\n",
    "\n",
    "        # Convert to HSV\n",
    "        img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "        \n",
    "        # Apply transformations\n",
    "        img_hsv[..., 1] *= saturation\n",
    "        img_hsv[..., 1] = np.clip(img_hsv[..., 1], 0, 255)\n",
    "        img_hsv[..., 2] *= brightness * contrast\n",
    "        img_hsv[..., 2] = np.clip(img_hsv[..., 2], 0, 255)\n",
    "\n",
    "        # Convert back to RGB\n",
    "        img = cv2.cvtColor(img_hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"creating-a-custom-loader\"></a>\n",
    "\n",
    "### üßê Inspecting Dataset using a **Custom Loader**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show an example of inspecting the dataset.  \n",
    "We **must** provide the width and height for the `BaseLoaderTorch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Function to denormalize the image\n",
    "def denormalize(img_tensor, mean, std):\n",
    "    img = img_tensor.clone()\n",
    "    for t, m, s in zip(img, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return img\n",
    "\n",
    "# Convert tensor to NumPy array for display\n",
    "def tensor_to_numpy(img_tensor):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    img = denormalize(img_tensor, mean, std)\n",
    "    # Convert from CxHxW to HxWxC\n",
    "    img = img.numpy().transpose(1, 2, 0)\n",
    "    # Clip and convert to uint8 (values in 0-255)\n",
    "    img = (img * 255).clip(0, 255).astype('uint8')\n",
    "    return img\n",
    "\n",
    "def plot_image_with_boxes(img_tensor, labels):\n",
    "    img = tensor_to_numpy(img_tensor['image'])\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Bounding boxes are stored as [class_id, xmin, ymin, xmax, ymax] in normalized coordinates\n",
    "    boxes = labels[\"/boundingbox\"].numpy()\n",
    "    for box in boxes:\n",
    "        class_id, xmin, ymin, xmax, ymax = box\n",
    "        # Convert from normalized to pixel coordinates\n",
    "        x = xmin * w\n",
    "        y = ymin * h\n",
    "        box_w = (xmax - xmin) * w\n",
    "        box_h = (ymax - ymin) * h\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (x, y),\n",
    "            box_w,\n",
    "            box_h,\n",
    "            linewidth=2,\n",
    "            edgecolor='red',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(\n",
    "            x,\n",
    "            y,\n",
    "            str(int(class_id)),\n",
    "            color='yellow',\n",
    "            bbox=dict(facecolor='red', alpha=0.5)\n",
    "        )\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "loader = VOCLoaderTorch(view=[\"train\"], height=512, width=512) # BaseLoaderTorch expects height and width to be set\n",
    "\n",
    "image, labels = loader[np.random.randint(0, len(loader))]\n",
    "\n",
    "plot_image_with_boxes(image, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"Ô∏èÔ∏ètraining\"></a>\n",
    "\n",
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Ô∏èconfiguration\"></a>\n",
    "\n",
    "### ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the training configuration for model training. Important notes:\n",
    "\n",
    "- We **must** set `trainer.preprocessing.train_image_size`, which defines the width and height for the `BaseLoaderTorch`, effectively telling the loader what size of images we will train with.\n",
    "- Additionally, it is important to set the normalization parameters, even though we are using our own custom implementation.  \n",
    "  This is necessary **only** for visualizations during training and does not affect the actual data loading, since we have our own custom loader.\n",
    "\n",
    "\n",
    "üëâ For the full list of all parameters, please check [Luxonis-Train](https://github.com/luxonis/luxonis-train/tree/main).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile detection_light_model.yaml\n",
    "model:\n",
    "  name: detection_light\n",
    "  predefined_model:\n",
    "    name: DetectionModel\n",
    "    params:\n",
    "      variant: light\n",
    "      loss_params:\n",
    "        iou_type: \"siou\"\n",
    "\n",
    "        # Should be 2.5 * accumulate_grad_batches for best results\n",
    "        iou_loss_weight: 20\n",
    "\n",
    "        # Should be 1 * accumulate_grad_batches for best results\n",
    "        class_loss_weight: 8\n",
    "\n",
    "loader:\n",
    "  test_view: val # there is no test set in VOC\n",
    "  name: VOCLoaderTorch\n",
    "  params:\n",
    "    root: ./data/VOCdevkit/\n",
    "    year: 2007\n",
    "  \n",
    "\n",
    "trainer:\n",
    "  preprocessing:\n",
    "    train_image_size: [384, 512] # Needed for the BaseLoaderTorch\n",
    "    normalize: # Needed just for the visualization denormalization in luxonis-train\n",
    "      active: true\n",
    "      params:\n",
    "        mean: [0.485, 0.456, 0.406]\n",
    "        std: [0.229, 0.224, 0.225]\n",
    "  \n",
    "  precision: \"16-mixed\"\n",
    "  batch_size: 8\n",
    "  epochs: 300\n",
    "  # For best results, always accumulate gradients to\n",
    "  # effectively use 64 batch size\n",
    "  accumulate_grad_batches: 8\n",
    "  n_workers: 8\n",
    "  validation_interval: 1\n",
    "  n_log_images: 50\n",
    "\n",
    "  callbacks:\n",
    "    - name: EMACallback\n",
    "      params:\n",
    "        decay: 0.9999\n",
    "        use_dynamic_decay: True\n",
    "        decay_tau: 2000\n",
    "    - name: ExportOnTrainEnd\n",
    "    - name: TestOnTrainEnd\n",
    "\n",
    "  training_strategy:\n",
    "    name: \"TripleLRSGDStrategy\"\n",
    "    params:\n",
    "      warmup_epochs: 2\n",
    "      warmup_bias_lr: 0.05\n",
    "      warmup_momentum: 0.5\n",
    "      lr: 0.0032\n",
    "      lre: 0.000384\n",
    "      momentum: 0.843\n",
    "      weight_decay: 0.00036\n",
    "      nesterov: True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"train\"></a>\n",
    "\n",
    "### ü¶æ Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To start the training, we need to initialize the `LuxonisModel`, pass it the path to the configuration file, and call the `train()` method on it.\n",
    "\n",
    " **Note**: LuxonisTrain also supports all these commands through its CLI ([documentation here](https://github.com/luxonis/luxonis-train/tree/main?tab=readme-ov-file#-cli)), no code required. For custom nodes, simply provide the `--source` flag with the path to where your custom components are initialized:\n",
    "\n",
    " ```bash\n",
    " luxonis_train --source custom_components.py train --config detection_ligth_model.yaml\n",
    " ```\n",
    "\n",
    "We won't use the CLI for this tutorial, but feel free to use it in your own projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luxonis_train import LuxonisModel\n",
    "\n",
    "path = \"./detection_light_model.yaml\"\n",
    "model = LuxonisModel(cfg=path)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LuxonisTrain` has also already implemented automatic tracking of training runs. By default, `Tensorboard` is used, and to look at the losses, metrics, and visualizations during training, we can inspect the logs. If you check the `output` folder, you'll see that every run creates a new directory, and each run also has its training logs in the `./output/tensorboard_logs` where the name of the folder matches the run's name. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
